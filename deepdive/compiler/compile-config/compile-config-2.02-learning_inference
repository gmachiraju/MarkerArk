#!/usr/bin/env jq
# compile-config-2.02-learning_inference -- Adds processes for performing inference with the grounded factor graph
##

include "constants";
include "sql";

# skip adding learning/inference processes unless one for grounding is there
if .deepdive_.execution.processes | has("process/grounding/combine_factorgraph") | not then . else

.deepdive_ as $deepdive
| .deepdive_.execution.processes += {

    # learning weights and doing inference (since we had to load the graph anyway)
    "process/model/learning": {
        dependencies_: ["model/factorgraph"],
        output_: ["model/weights", "data/model/weights"],
        style: "cmd_extractor",
        cmd: "
        export DEEPDIVE_LOAD_FORMAT=tsv
        : ${DEEPDIVE_NUM_PROCESSES:=$(nproc --ignore=1)}
        : ${DEEPDIVE_RESULTS_DIR:=\"$DEEPDIVE_APP\"/run/model/results}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        flatten() { find -L \"$@\" -type f -size +0 -exec pbzip2 -c -d -k {} +; }

        for pid in $(seq 0 \($deepdive.sampler.partitions - 1)); do
            mkdir -p $DEEPDIVE_RESULTS_DIR/$pid
            cd $DEEPDIVE_RESULTS_DIR/$pid

            FGDIR=$DEEPDIVE_FACTORGRAPH_DIR/$pid

            [ -d $FGDIR ] || error \"No factorgraph found\"

            # run inference engine for learning and inference

            if [ $pid -gt 0 ]; then
                # use weights learned from previous shard
                WEIGHT_DIR=weights_hot
            else
                WEIGHT_DIR=weights
            fi

            # if there are multiple shards, we suppress inference (-i 0)

            \(if $deepdive.sampler.sampler_cmd == "numbskull" then "
                mkdir -p $FGDIR/flat
                head -n 1 $FGDIR/meta > $FGDIR/flat/graph.meta
                # TODO: save disk space
                flatten $FGDIR/variables > $FGDIR/flat/graph.variables
                flatten $FGDIR/factors > $FGDIR/flat/graph.factors
                flatten $FGDIR/$WEIGHT_DIR > $FGDIR/flat/graph.weights
                flatten $FGDIR/domains > $FGDIR/flat/graph.domains
                numbskull $FGDIR/flat \\
                    --output_dir . \\
                    --threads $DEEPDIVE_NUM_PROCESSES \\
                    \($deepdive.sampler.sampler_args // "") \\
                    \(if $deepdive.sampler.partitions > 1 then "-i 0" else "" end) \\
                    #
            " else "
                \($deepdive.sampler.sampler_cmd) \\
                    gibbs \\
                    --variables <(flatten $FGDIR/variables) \\
                    --domains <(flatten $FGDIR/domains) \\
                    --factors <(flatten $FGDIR/factors) \\
                    --weights <(flatten $FGDIR/$WEIGHT_DIR) \\
                    --fg_meta $FGDIR/meta \\
                    --outputFile . \\
                    --n_threads $DEEPDIVE_NUM_PROCESSES \\
                    \($deepdive.sampler.sampler_args // "") \\
                    \(if $deepdive.sampler.partitions > 1 then "-i 0" else "" end) \\
                    #
            " end)

            # load weights to database
            deepdive create table \(deepdiveInferenceResultWeightsTable | @sh) \\
                wid:BIGINT:'PRIMARY KEY' \\
                weight:'DOUBLE PRECISION' \\
                #
            cat inference_result.out.weights.text |
            tr \(" "|@sh) \("\\t"|@sh) |
            deepdive load \(deepdiveInferenceResultWeightsTable | @sh) /dev/stdin
            deepdive db analyze \(deepdiveInferenceResultWeightsTable | @sh)
            mv inference_result.out.weights.text inference_result.out.weights.text.learned

            \(if $deepdive.sampler.partitions > 1 then "
                # dump weights back out for next-shard learning or inference step
                if [ $pid -lt \($deepdive.sampler.partitions - 1) ]; then
                    # not last shard: dump for next-shard learning
                    export TARGET_WEIGHT_DIR=$DEEPDIVE_FACTORGRAPH_DIR/$(($pid+1))/weights_hot
                else
                    export TARGET_WEIGHT_DIR=$DEEPDIVE_FACTORGRAPH_DIR/learned_weights
                fi

                mkdir -p $TARGET_WEIGHT_DIR
                deepdive compute execute \\
                    input_sql=\(
                        { SELECT:
                            [ { table: "w", column: "wid" }
                            , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                            , { expr: "COALESCE(r.weight, w.initvalue, 0)" }
                            ]
                        , FROM: [ { alias: "w", table: deepdiveGlobalWeightsTable } ]
                        , JOIN: { LEFT_OUTER: { alias: "r", table: deepdiveInferenceResultWeightsTable }
                                , ON: { eq: [ { table: "r", column: "wid" }
                                            , { table: "w", column: "wid" }
                                            ] }
                                }
                        } | asSql | asPrettySqlArg) \\
                    command=\("
                        sampler-dw text2bin weight /dev/stdin /dev/stdout /dev/null | pbzip2 >$TARGET_WEIGHT_DIR/weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2
                    " | @sh) \\
                    output_relation=

            " else "" end)

        done

        deepdive create view \(deepdiveInferenceResultWeightsMappingView | @sh) as \(
            { SELECT:
                [ { expr: "\"w\".*" }
                , { table: "r", column: "weight" }
                ]
            , FROM: { alias: "w", table: deepdiveGlobalWeightsTable }
            , JOIN: { INNER: { alias: "r", table: deepdiveInferenceResultWeightsTable }
                    , ON: { eq: [ { table: "r", column: "wid" }
                                , { table: "w", column: "wid" }
                                ] } }
            , ORDER_BY: { expr: "ABS(\"r\".\"weight\")", order: "DESC" }
            } | asSql | asPrettySqlArg)
        "
    },

    # performing inference
    "process/model/inference": {
        dependencies_: ["model/factorgraph", "model/weights"],
        output_: ["model/probabilities"],
        style: "cmd_extractor",
        cmd: "
        : ${DEEPDIVE_NUM_PROCESSES:=$(nproc --ignore=1)}
        : ${DEEPDIVE_RESULTS_DIR:=\"$DEEPDIVE_APP\"/run/model/results}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        flatten() { find -L \"$@\" -type f -size +0 -exec pbzip2 -c -d -k {} +; }

        # need to run inference only if learning step was deferred because there are multiple shards
        \(if $deepdive.sampler.partitions > 1 then "
            for pid in $(seq 0 \($deepdive.sampler.partitions - 1)); do

                mkdir -p $DEEPDIVE_RESULTS_DIR/$pid
                cd $DEEPDIVE_RESULTS_DIR/$pid

                FGDIR=$DEEPDIVE_FACTORGRAPH_DIR/$pid

                [ -d $FGDIR ] || error \"No factorgraph found\"

                # XXX this skipping may cause confusion
                # run sampler for inference with given weights without learning
                LEARNED_WEIGHTS=$DEEPDIVE_FACTORGRAPH_DIR/learned_weights

                \(if $deepdive.sampler.sampler_cmd == "numbskull" then "
                    mkdir -p $FGDIR/flat
                    if [ $FGDIR/meta -nt $FGDIR/flat/graph.meta ]; then
                        head -n 1 $FGDIR/meta > $FGDIR/flat/graph.meta
                        flatten $FGDIR/variables > $FGDIR/flat/graph.variables
                        flatten $FGDIR/factors > $FGDIR/flat/graph.factors
                        flatten $FGDIR/domains > $FGDIR/flat/graph.domains
                    fi
                    flatten $LEARNED_WEIGHTS > $FGDIR/flat/graph.weights
                    numbskull $FGDIR/flat \\
                        --output_dir . \\
                        --threads $DEEPDIVE_NUM_PROCESSES \\
                        \($deepdive.sampler.sampler_args // "") \\
                        -l 0 \\
                        #
                " else "
                    \($deepdive.sampler.sampler_cmd) \\
                        gibbs \\
                        --variables <(flatten $FGDIR/variables) \\
                        --domains <(flatten $FGDIR/domains) \\
                        --factors <(flatten $FGDIR/factors) \\
                        --weights <(flatten $LEARNED_WEIGHTS) \\
                        --fg_meta $FGDIR/meta \\
                        --outputFile . \\
                        --n_threads $DEEPDIVE_NUM_PROCESSES \\
                        \($deepdive.sampler.sampler_args // "") \\
                        -l 0 \\
                        #
                " end)

            done

        " else "
            echo Skipping process/model/inference because it was inlined in process/model/learning
        " end)
        "
    },

    "process/model/load_probabilities": {
        dependencies_: ["model/probabilities"],
        output_: ["data/model/probabilities"],
        style: "cmd_extractor",
        cmd: "
            : ${DEEPDIVE_RESULTS_DIR:=\"$DEEPDIVE_APP\"/run/model/results}

            # load weights to database
            deepdive create table \(deepdiveInferenceResultVariablesTable | @sh) \\
                vid:BIGINT \\
                cid:BIGINT \\
                prb:'DOUBLE PRECISION' \\
                #

            # restore shard ID to the vids
            for pid in $(seq 0 \($deepdive.sampler.partitions - 1)); do
                cd $DEEPDIVE_RESULTS_DIR/$pid

                AWK_CMD='{printf \"%s\\t%s\\t%s\\n\", SHARD_BASE + $1, $2, $3}'

                cat inference_result.out.text |
                awk -v SHARD_BASE=$(($pid << 48)) \"$AWK_CMD\" |
                DEEPDIVE_LOAD_FORMAT=tsv \\
                deepdive load \(deepdiveInferenceResultVariablesTable | @sh) /dev/stdin
            done

            deepdive db analyze \(deepdiveInferenceResultVariablesTable | @sh)

            # create a view for each app schema variable
            \([ $deepdive.schema.variables_[] | "
                deepdive create view \("\(.variablesTable)_inference" | @sh) as \(
                { SELECT:
                    [ { expr: "\"v\".*" }
                    , { alias: deepdiveVariableExpectationColumn,   table: "r", column: "prb" }
                    , { alias: deepdiveVariableInternalLabelColumn, table: "i", column: deepdiveVariableInternalLabelColumn }
                    , { alias: deepdiveVariableIdColumn,            table: "i", column: deepdiveVariableIdColumn }
                    ]
                , FROM: [ { alias: "v", table: .variablesTable } ]
                , JOIN:
                    # variable ids
                    [ { LEFT_OUTER: { alias: "i", table: .variablesIdsTable }
                      , ON: { and:  [ .variablesKeyColumns[]
                                    | { eq: [ { table: "i", column: . }
                                            , { table: "v", column: . }
                                            ] }
                                    ] } }
                    , if .variableType == "boolean" then
                    # inference results
                      { LEFT_OUTER: { alias: "r", table: deepdiveInferenceResultVariablesTable }
                      , ON: { eq:   [ { table: "r", column: "vid" }
                                    , { table: "i", column: deepdiveVariableIdColumn }
                                    ] } }
                    else
                    # category ids are necessary to find the inference result corresponding to the variable
                      { LEFT_OUTER: { alias: "c", table: .variablesCategoriesTable }
                      , ON: { and:  [ .variablesCategoryColumns[]
                                    | { eq: [ { table: "c", column: "_\(.)" }
                                            , { table: "v", column: . }
                                            ] }
                                    ] } }
                    # inference results
                    , { LEFT_OUTER: { alias: "r", table: deepdiveInferenceResultVariablesTable }
                      , ON: { and:  [ { eq: [ { table: "r", column: "vid" }
                                            , { table: "i", column: deepdiveVariableIdColumn }
                                            ] }
                                    , { eq: [ { table: "r", column: "cid" }
                                            , { table: "c", column: "cid" }
                                            ] }
                                    ] } }
                    end
                    ]
                , ORDER_BY:
                    { expr: { table: "r", column: "prb" }
                    , order: "DESC"
                    }
                } | asSql | @sh)"
            ] | join("\n"))
        "
    }

}

end
