#!/usr/bin/env jq
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##

include "constants";
include "util";
include "sql";

# skip adding grounding processes unless there are variables and factors defined
if (.deepdive_.schema.variables_  | length) == 0
or (.deepdive_.inference.factors_ | length) == 0
then . else

def factorWeightDescriptionSqlExpr:
    [ ("\(.factorName)-" | asSqlLiteral)
    , (.weight_.params[] |
        "CASE WHEN \(asSqlIdent) IS NULL THEN ''
              ELSE \(asSqlIdent) || ''  -- XXX CAST(... AS TEXT) unsupported by MySQL
          END"
      )
    ] | join(" ||\("-" | asSqlLiteral)|| ");

.deepdive_ as $deepdive

###############################################################################

## variable_id_partition
# Grounding begins by counting the variables to partition a range of
# non-negative integers for assigning the variable ids.
| .deepdive_.execution.processes += {
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all variable tables
            $deepdive.schema.variables_[] | "data/\(.variablesTable)"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables_ | map(.variablesTable | @sh) | join(" ")) | {
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    }
}


## variable/*/assign_id
# Each variable table then gets the range of integers assigned to the id column
# of every row.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/assign_id": {
        dependencies_: [
            "process/grounding/variable_id_partition"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        table=\(.variablesTable | @sh)

        cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
        baseId=$(cat id_begin)

        # assign id to all rows according to the paritition
        deepdive db assign_sequential_id $table \(deepdiveVariableIdColumn | @sh) $baseId

        \(
        if .variableType == "categorical" then
            "
            # generate a table holding all categories for categorical variables
            # that can be cross joined with weights tables for factors over
            # those variables
            deepdive db generate_series \(.variableCategoriesTable | @sh) category 0 \(.variableCardinality - 1 | tostring)
            "
        else "" end
        )
        "
    }
})

## variable_holdout
# Variables to holdout are recorded by executing either a user-defined
# (app-wide) holdout query, or by taking a random sample of a user-defined
# fraction.
# TODO easier way to do holdout per variable?
| .deepdive_.execution.processes += {
    "process/grounding/variable_holdout": {
        dependencies_: [
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        deepdive create table \(deepdiveGlobalHoldoutTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        deepdive create table \(deepdiveGlobalObservationTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        \([ if $deepdive.calibration.holdout_query then
                # run user holdout query if configured
                $deepdive.calibration.holdout_query
          else
            # otherwise, randomly select from evidence variables of each variable table
            $deepdive.schema.variables_[] | "
                INSERT INTO \(deepdiveGlobalHoldoutTable | asSqlIdent) \(
                { SELECT:
                    [ { column: deepdiveVariableIdColumn }
                    ]
                , FROM:
                    [ { table: .variablesTable }
                    ]
                , WHERE:
                    [ { isntNull: { column: .variablesLabelColumn } }
                    , { lt: [ { expr: "RANDOM()" }
                            , { expr: $deepdive.calibration.holdout_fraction }
                            ]
                      }
                    ]
                } | asSql);
            "
          end
        , if $deepdive.calibration.observation_query then
            # run user holdout query if configured
            $deepdive.calibration.holdout_query
          else empty
          end
        ] | map("deepdive sql \(@sh)") | join("\n"))
        "
    }
}

## variable/*/dump
# Then each variable table is dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/dump": {
        dependencies_: [
            "process/grounding/variable_holdout"
          # XXX below can be omitted for now
          #, "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        table=\(.variablesTable | @sh)

        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        find . -name 'variables.part-*.bin.bz2' -exec rm -rf {} +
        export DEEPDIVE_LOAD_FORMAT=tsv
        export DEEPDIVE_UNLOAD_MATERIALIZED=false

        # dump the variables, joining the holdout query to determine the type of each variable
        deepdive compute execute \\
            input_sql=\(
            { SELECT:
                [ { column: "id" }
                , { column: "variable_role" }
                , { alias: "init_value", expr:
                    "CASE WHEN variable_role = 0 THEN 0
                          ELSE (\(
                            if   .variableType == "boolean"     then "CASE WHEN label THEN 1 ELSE 0 END" # XXX a portable way to turn boolean to integers in SQL, CAST(label AS INT) does not work for MySQL
                            elif .variableType == "categorical" then "label"
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end
                            )) + 0.0
                      END" }
                , { column: "variable_type" }
                , { column: "cardinality" }
                ]
            , FROM:
                [ { alias: "variables", sql:
                    { SELECT:
                        [ { alias: "id", column: deepdiveVariableIdColumn }
                        , { alias: "variable_role", expr:
                              "CASE WHEN               observation.variable_id IS NOT NULL
                                     AND variables.\(.variablesLabelColumn | asSqlIdent) IS NOT NULL THEN 2
                                    WHEN               holdout.variable_id IS NOT NULL THEN 0
                                    WHEN variables.\(.variablesLabelColumn | asSqlIdent) IS NOT NULL THEN 1
                                                                                       ELSE 0
                                END" }
                        , { alias: "label", table: "variables", column: .variablesLabelColumn }
                        , { alias: "variable_type", expr: (
                                if   .variableType == "boolean"     then 0
                                elif .variableType == "categorical" then 1
                                else error("Internal error: Unknown variableType: \(.variableType)")
                                end
                            ) }
                        , { alias: "cardinality", expr: (
                                .variableCardinality
                            ) }
                        ]
                    , FROM:
                        [ { alias: "variables", table: .variablesTable }
                        ]
                    , JOIN:
                        [ { LEFT_OUTER:
                            { alias: "holdout"
                            , table: deepdiveGlobalHoldoutTable
                            }
                          , ON: { eq:
                                    [ { table: "variables", column: deepdiveVariableIdColumn }
                                    , { table: "holdout"  , column: "variable_id" }
                                    ]
                                }
                          }
                        , { LEFT_OUTER:
                            { alias: "observation"
                            , table: deepdiveGlobalObservationTable
                            }
                          , ON: { eq:
                                    [ { table: "variables"  , column: deepdiveVariableIdColumn }
                                    , { table: "observation", column: "variable_id" }
                                    ]
                                }
                          }
                        ]
                    }
                  }
                ]
            } | asSql | @sh) \\
            command=\("
                format_converter variable /dev/stdin >(pbzip2 >variables.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
            " | @sh) \\
            output_relation=
        "
    }

})


###############################################################################

## factor/*/materialize
# Each inference rule's SQL query is run to materialize the factors and the
# distinct weights used in them.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors
    "process/grounding/factor/\(.factorName)/materialize": {
        # materializing each factor requires the dependent variables to have their id assigned
        dependencies_: [
            .input_[]
            | ltrimstr("data/")
            | $deepdive.schema.variables_byName[.]
            | select(type != "null")
            # the involved variables must have their ids all assigned
            | "process/grounding/variable/\(.variableName)/assign_id"?
        ],
        # other non-variable tables are also necessary
        input_: [ .input_[]
            | select(ltrimstr("data/") | in($deepdive.schema.variables_byName) | not)
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            # materialize factors using user input_query that pulls in assigned ids to involved variables
            deepdive create table \(.factorsTable | @sh) as \(.input_query | @sh)

            # find distinct weights for the factors into a separate table
            deepdive create table \(.weightsTable | @sh) as \(
                { SELECT:
                    [ ( .weight_.params[] | { column: . } )
                    , { alias: "isfixed"  , expr: .weight_.is_fixed   }
                    , { alias: "initvalue", expr: .weight_.init_value }
                    , { alias: "id"       , expr: -1                  }  # TODO cast to BIGINT?
                    ]
                # when weight is parameterized, find all distinct ones
                , FROM:
                    (if .weight_.params | length == 0 then [] else
                        [ { table: .factorsTable }
                        ]
                    end)
                , GROUP_BY:
                    (if .weight_.params | length == 0 then [] else
                        [ ( .weight_.params[] | { column: . } )
                        ]
                    end)
                } | asSql | @sh)
        "
    }
})


## weight_id_partition
# The weight ids must be first partitioned by counting them.
| .deepdive_.execution.processes += {
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # partition the id range for weights
        RANGE_BEGIN=0 RANGE_STEP=1 \\
        partition_id_range \([ $deepdive.inference.factors_[] | "\(.weightsTable | @sh)\(
                if .function_.name != "multinomial" then ""
                else ":$(( \([ .function_.variables[]
                             | .schema.variableCardinality
                             | tostring ] | join("*")) ))"
                    # TODO switch to bc to handle arbitrarily large numbers
                end)" ] | join(" ")) | {
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=${table#\(deepdivePrefixForWeightsTable | @sh)}
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }
        "
    }
}

## global_weight_table
# To view the weights learned by the inference engine later, set up an app-wide table.
| .deepdive_.execution.processes += {
    "process/grounding/global_weight_table": {
        dependencies_: [
            $deepdive.inference.factors_[] |
            if .function_.name == "multinomial" then
                "process/grounding/factor/\(.factorName)/assign_weight_id"
            else
                "process/grounding/factor/\(.factorName)/materialize"
            end
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # set up a union view for all weight tables (\(deepdiveGlobalWeightsTable | asSqlIdent))
        deepdive create view \(deepdiveGlobalWeightsTable | @sh) as \(
            [ $deepdive.inference.factors_[] |
                { SELECT:
                    [ { column: "id" }
                    , { column: "isfixed" }
                    , { column: "initvalue" }
                    , { alias: "description", expr: factorWeightDescriptionSqlExpr
                      }
                    , if .function_.name != "multinomial" then
                        # TODO maybe '1' is a better one for boolean variables?
                        { alias: "categories", expr: "NULL" }
                    else # multinomial factor weights table have a "categories" column
                        { column: "categories" }
                    end
                    ]
                , FROM:
                    [ { table: .weightsTableForDumping }
                    ]
                } | asSql | "(\(.))"
            ] | join("\nUNION ALL\n") | @sh)
        "
    }
}

## factor/*/assign_weight_id
# Each inference rule gets its weight ids actually assigned.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    "process/grounding/factor/\(.factorName)/assign_weight_id": {
        dependencies_: [
            "process/grounding/weight_id_partition"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            baseId=$(cat weights_id_begin)
            inc=1
            \( if .function_.name != "multinomial" then ""
            else
                "# since this is a multinomial factor, each factor will have as many weights as the product of cardinalities of all involved variables
                \( [ .function_.variables[] |
                    # TODO switch to bc to handle arbitrarily large numbers
                    "let inc*=\(.schema.variableCardinality | tostring)  # cardinality of variable \(.name)"
                ] | join("\n") )"
            end)

            # assign weight ids according to the partition
            deepdive db assign_sequential_id \(.weightsTable | @sh) id $baseId $inc

            \( if .function_.name != "multinomial" then "" else "
            # set up an exploded weights view that cross joins the base weights table with the categories of categorical variables
            deepdive create view \(.weightsTableForDumping | @sh) as \(
                { SELECT:
                    [ ( .weight_.params[]
                    | { column: .          , table: "weights" } )
                    , { column: "isfixed"  , table: "weights" }
                    , { column: "initvalue", table: "weights" }
                    # fill in the ids of weights for every combination of categories
                    , { alias: "id", expr: "\"weights\".\"id\" + (\(
                            def offsetExpr(i): .[i].schema as $v |
                                if i == 0 then "" # except the very first variable
                                else # the offset for the previous ones are multiplied by this one's cardinality
                                    "(\(offsetExpr(i-1)))*\($v.variableCardinality | tostring) + "
                                end
                                # and current variable's category number is added as the least significant digit
                                + "\"c\(i)\".\"category\""
                                ;
                            .function_.variables | offsetExpr(length-1)
                        ))" }
                    # generate a category combination identifier
                    , { alias: "categories", expr: (
                        .function_.variables | if length > 1 then
                            [ .[] | "\"c\(.ordinal)\".\"category\"" ] | join(" || ',' || ")
                        else
                            "\"c0\".\"category\" || ''"
                        end)
                      }
                    ]
                , FROM:
                    [ { alias: "weights", table: .weightsTable }
                    # cross join each weight with the categories of all involved categorical variables
                    , ( .function_.variables[]
                    | { alias: "c\(.ordinal)", table: .schema.variableCategoriesTable } )
                    ]
                } | asSql | @sh)"
            end)
        "
    }
})


## factor/*/dump
# The factors are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'factors.part-*.bin.bz2' \\
                    -o -name 'nfactors.part-*'         \\
                    -o -name   'nedges.part-*'         \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # dump the factors joining the assigned weight ids, converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\(
                    { SELECT:
                        [ { table: "weights", column: "id", alias: "weight_id" }
                        , ( .function_.variables[] |
                            { table: "factors", column: .columnId }
                          )
                        ]
                    , FROM:
                        [ { table: .factorsTable, alias: "factors" }
                        , { table: .weightsTable, alias: "weights" }
                        ]
                    , WHERE:
                        [ .weight_.params[] |
                            { eq: [ { table: "factors", column: . }
                                  , { table: "weights", column: . }
                                  ]
                            }
                        ]
                    } | asSql | @sh) \\
                command=\("
                    # also record the factor count
                    tee >(wc -l >nfactors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}) |
                    format_converter factor /dev/stdin >(pbzip2 >factors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2) \(.function_.id
                        ) \(.function_.variables | length
                        ) original \(.function_.variables | map(if .isNegated then "0" else "1" end) | join(" ")
                        ) |
                    # and the edge count
                    tee nedges.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}
                " | @sh) \\
                output_relation=
        "
    }
})

## factor/*/dump_weights
# The factors and weights are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump_weights": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'weights.part-*.bin.bz2' \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # flag that signals whether to reuse weights or not
            reuseFlag=\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights.reuse

            # dump the weights (except the description column), converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\"$(if [[ -e \"$reuseFlag\" ]]; then
                    echo \(
                    # dump weights with initvalue from previously learned ones
                    { SELECT:
                        [ { table: "w", column: "id" }
                        , { expr: "CASE WHEN w.isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(reuse.weight, w.initvalue, 0)" }
                        ]
                    , FROM: [ { alias: "w", table: .weightsTableForDumping } ]
                    , JOIN: { LEFT_OUTER: { alias: "reuse", table: deepdiveReuseWeightsTable }
                            , ON: { and: [ { eq: [ { table: "reuse", column: "description" }
                                                 , { expr: factorWeightDescriptionSqlExpr }
                                                 ] }
                                         , if .function_.name == "multinomial" then
                                           { or: [ { isNull: { table: "reuse", column: "categories" } }
                                                   , { eq: [ { table: "reuse", column: "categories" }
                                                           , { table: "w",     column: "categories" }
                                                           ] }
                                                 ] }
                                           else empty end
                                         ] }
                            }
                    } | asSql | @sh)
                else
                    echo \(
                    # dump weights from scratch
                    { SELECT:
                        [ { column: "id" }
                        , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(initvalue, 0)" }
                        ]
                    , FROM: [ { table: .weightsTableForDumping } ]
                    } | asSql | @sh)
                fi)\" \\
                command=\("
                    format_converter weight /dev/stdin >(pbzip2 >weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
                " | @sh) \\
                output_relation=
        "
    }
})


###############################################################################

# Finally, put together everything dumped into a layout the inference engine can easily load from
| .deepdive_.execution.processes += {
    "process/grounding/combine_factorgraph": {
        dependencies_: [(
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/dump"
        ), (
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/dump"
            , "process/grounding/factor/\(.factorName)/dump_weights"
        ), (
            "process/grounding/global_weight_table"
        )],
        output_: "model/factorgraph",
        style: "cmd_extractor", cmd: (
            ([$deepdive.schema.variables_[] | .variableName | @sh] | join(" ")) as $variableNames |
            ([$deepdive.inference.factors_[] | .factorName  | @sh] | join(" ")) as $factorNames   |
        "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        # create a fresh empty directory for the new combined factor graph
        rm -rf   \"$DEEPDIVE_FACTORGRAPH_DIR\"
        mkdir -p \"$DEEPDIVE_FACTORGRAPH_DIR\"
        cd \"$DEEPDIVE_FACTORGRAPH_DIR\"

        # create symlinks to the grounded binaries by enumerating variables and factors
        for v in \($variableNames); do
            mkdir -p variables/\"$v\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/variable/\"$v\" \\
                -name 'variables.part-*.bin.bz2' -exec ln -sfnv -t variables/\"$v\"/ {} + \\
                #
        done
        for f in \($factorNames); do
            mkdir -p {factors,weights}/\"$f\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\" \\
                -name 'factors.part-*.bin.bz2' -exec ln -sfnv -t factors/\"$f\"/ {} + \\
                -o \\
                -name 'weights.part-*.bin.bz2' -exec ln -sfnv -t weights/\"$f\"/ {} + \\
                #
        done

        # generate the metadata for the inference engine
        {
            # first line with counts of variables and edges in the grounded factor graph
            cd \"$DEEPDIVE_GROUNDING_DIR\"
            sumup() { { tr '\\n' +; echo 0; } | bc; }
            counts=()
            counts+=($(cat factor/weights_count))
            # sum up the number of factors and edges
            counts+=($(cat variable_count))
            cd factor
            counts+=($(find \($factorNames) -name 'nfactors.part-*' -exec cat {} + | sumup))
            counts+=($(find \($factorNames) -name 'nedges.part-*'   -exec cat {} + | sumup))
            (IFS=,; echo \"${counts[*]}\")
            # second line with file paths
            paths=(\"$DEEPDIVE_FACTORGRAPH_DIR\"/{weights,variables,factors,edges})
            (IFS=,; echo \"${paths[*]}\")
        } >meta
        ")
    }
}

end
