{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import BiomarkerDrugRelation_Extraction\n",
    "import cPickle, os, sys\n",
    "from snorkel.ddlite import *\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (18,6)\n",
    "filename = \"text/AGR2_blood_biomarker.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing documents...\n",
      "Parsing contexts...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File AGR2_blood_biomarker too long. Max character count is 100K",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3b7d5e1c1ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiomarkerDrugRelation_Extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateRelations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/kamalasistla/anaconda/envs/venv/bin/Marker-Reader-master/Marker-Reader/Marker-Reader/BiomarkerDrugRelation_Extraction.pyc\u001b[0m in \u001b[0;36mgenerateRelations\u001b[0;34m(_filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdoc_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDocParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msent_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# print corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Sentences havent been parsed, so parse them now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kamalasistla/anaconda/envs/venv/bin/Marker-Reader-master/Marker-Reader/Marker-Reader/snorkel/snorkel/parser.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, doc_parser, context_parser, max_docs)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contexts_by_id\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contexts_by_doc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docs_by_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contexts_by_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contexts_by_doc_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kamalasistla/anaconda/envs/venv/bin/Marker-Reader-master/Marker-Reader/Marker-Reader/snorkel/snorkel/parser.pyc\u001b[0m in \u001b[0;36mparse_docs\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kamalasistla/anaconda/envs/venv/bin/Marker-Reader-master/Marker-Reader/Marker-Reader/snorkel/snorkel/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, s, doc_id, doc_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Request is too long\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CoreNLP request timed out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File {} too long. Max character count is 100K\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0msent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File AGR2_blood_biomarker too long. Max character count is 100K"
     ]
    }
   ],
   "source": [
    "Relations = BiomarkerDrugRelation_Extraction.generateRelations(_filename= filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = None\n",
    "relationsFile = \"cache/\" + filename + \"/relations.pkl\"\n",
    "if not os.path.exists(\"cache/\" + filename +\"/\" ):\n",
    "    os.makedirs(\"cache/\" + filename + \"/\")\n",
    "try:\n",
    "    with open(relationsFile, 'rb') as f:\n",
    "        feats = cPickle.load(f)\n",
    "except:\n",
    "    %time Relations.extract_features()\n",
    "    with open(relationsFile, 'w+') as f:\n",
    "        cPickle.dump(Relations.feats, f)\n",
    "\n",
    "DDL = DDLiteModel(Relations, feats)\n",
    "print \"Extracted {} features for each of {} mentions\".format(DDL.num_feats(), DDL.num_candidates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('databases/uids.pkl', 'rb') as f:\n",
    "    uids = cPickle.load(f)\n",
    "with open('databases/gt.pkl', 'rb') as f:\n",
    "    gt = cPickle.load(f)\n",
    "\n",
    "# with open('examples/gene_tag_example/gt/uids.pkl', 'rb') as f:\n",
    "#     uids2 = cPickle.load(f)\n",
    "# with open('examples/gene_tag_example/gt/gt.pkl', 'rb') as f:\n",
    "#     gt2 = cPickle.load(f)\n",
    "    \n",
    "# print \"UIDS \\n\"\n",
    "# print uids\n",
    "# print type(uids)\n",
    "# print \"\\n\"\n",
    "# print \"UIDS2 \\n\"\n",
    "# print uids2\n",
    "# print type(uids2)\n",
    "# print \"\\n\"\n",
    "# print \"GT \\n\"\n",
    "# print gt\n",
    "# print type(gt)\n",
    "# print \"\\n\"\n",
    "# print \"GT2 \\n\"\n",
    "# print gt2\n",
    "# print type(gt2)\n",
    "# print \"\\n\"\n",
    "DDL.update_gt(gt = gt[:50], uids = uids[:50])\n",
    "DDL.set_holdout(validation_frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.open_mindtagger(num_sample=100, width='100%', height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.add_mindtagger_tags()\n",
    "DDL.update_gt(gt=gt[50:], uids=uids[50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyWords = ['basis', 'target', 'treat', 'treatment', 'inhibit', 'inhibition', 'inhibitor', ]\n",
    "negationWords = [\"not\", \"nor\", \"neither\"]\n",
    "\n",
    "def presenceOfNot(m):\n",
    "    for word in negationWords:\n",
    "        if (word in m.post_window1('lemmas', 20)) and (word in m.pre_window2('lemmas', 20)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 1\n",
    "def LF_distance(m):\n",
    "    # if 'neuroendocrine' in m.lemmas:\n",
    "    #     print m.lemmas \n",
    "    # print m.dep_labels\n",
    "    distance = abs(m.e2_idxs[0] - m.e1_idxs[0])\n",
    "    count = 0\n",
    "    for lemma in m.lemmas:\n",
    "        if lemma == ',':\n",
    "            count += 1\n",
    "    if count > 1 and ',' in m.pre_window1('lemmas', 1):\n",
    "        print m\n",
    "        return 0\n",
    "    if distance == 0:\n",
    "        return -1\n",
    "    if distance < 8:\n",
    "        # print \"RETURNING ONE\"\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def LF_keyword(m):\n",
    "    for word in keyWords:\n",
    "        if ((word in m.post_window1('lemmas', 20)) and (word in m.pre_window2('lemmas', 20))) or ((word in m.post_window1('lemmas', 20)) and (word in m.pre_window2('lemmas', 20))):\n",
    "            if presenceOfNot(m):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def LF_usedToTreat(m):\n",
    "     if ('used' in m.pre_window1('lemmas'), 20) and ('to' in m.pre_window1('lemmas', 20)) and ('treat' in m.pre_window1('lemmas', 20)):\n",
    "         return 1\n",
    "     else:\n",
    "         return 0\n",
    "     \n",
    "def LF_usedToInhibit(m):\n",
    "     if ('used' in m.pre_window1('lemmas'), 20) and ('to' in m.pre_window1('lemmas', 20)) and ('inhibit' in m.pre_window1('lemmas', 20)):\n",
    "         return 1\n",
    "     else:\n",
    "         return 0\n",
    "     \n",
    "def LF_inhibit(m):\n",
    "    if ('inhibit' in m.pre_window1('lemmas', 20) and 'inhibit' in m.pre_window2('lemms', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def LF_target(m):\n",
    "    if ('target' in m.pre_window1('lemmas', 20) and 'target' in m.pre_window2('lemms', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def LF_inhibitionOfWith(m):\n",
    "    if ('inhibition' in m.pre_window1('lemmas', 20) and 'of' in m.pre_window1('lemmas', 20) and 'with' in m.post_window1('lemmas', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def LF_isATreatmentOf(m):\n",
    "    if ('is' in m.pre_window1('lemmas', 20) and 'a' in m.pre_window1('lemmas', 20) and 'treatment' in m.pre_window1('lemmas', 20) and 'of' in m.pre_window1('lemmas', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def LF_antibody(m):\n",
    "    if ('antibody' in m.post_window2('lemmas', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def LF_hadisbad(m):\n",
    "    if(m.mention1 == \"had\" or m.mention2 == \"had\"):\n",
    "        return -1\n",
    "def LF_duration(m):\n",
    "    if(m.mention1 == \"duration\" or m.mention2 == \"duration\"):\n",
    "        return -1\n",
    "def LF_isaBiomarker(m):\n",
    "    post_window1_lemmas = m.post_window1('lemmas',20)\n",
    "    pre_window2_lemmas = m.pre_window2('lemmas',20)\n",
    "    if ('biomarker' in post_window1_lemmas and 'biomarker' in pre_window2_lemmas) or ('marker' in post_window1_lemmas and 'marker' in pre_window2_lemmas) or ('indicator' in post_window1_lemmas and 'indicator' in pre_window2_lemmas):\n",
    "        marker_idx_post_window1 = -1\n",
    "        markers = ['biomarker','marker','indicator']\n",
    "        for marker in markers:\n",
    "            try:\n",
    "                # print post_window1_lemmas\n",
    "                findMarker = post_window1_lemmas.index(marker)\n",
    "                if not findMarker == -1:\n",
    "                    marker_idx_post_window1 = findMarker\n",
    "                    print marker\n",
    "            except:\n",
    "                pass\n",
    "        if 'cop' in m.post_window1('dep_labels',20):\n",
    "            try:\n",
    "                cop_idx_post_window1 = m.post_window1('dep_labels',20).index('cop')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print \"MarkerIdx:\"\n",
    "            print marker_idx_post_window1\n",
    "            print \"ROOTIdx:\"\n",
    "            try:\n",
    "                print  m.post_window1('dep_labels',marker_idx_post_window1)\n",
    "                print  m.post_window1('dep_labels',marker_idx_post_window1).index('ROOT')\n",
    "            except:\n",
    "                pass\n",
    "            print '\\n'\n",
    "            \n",
    "            return 1 if ('nsubj' in m.mention1(attribute='dep_labels')) and (marker_idx_post_window1-cop_idx_post_window1 < 4)  else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [LF_distance, LF_keyword, LF_usedToTreat, LF_inhibit, LF_inhibitionOFWith, LF_isaBiomarker]\n",
    "DDL.apply_lfs(LFs, clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.lowest_empirical_accuracy_lfs(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "mu_seq = np.ravel([1e-9, 1e-5, 1e-3, 1e-1])\n",
    "lf_opts = {'sample': False, 'verbose': True}\n",
    "model_opts = {'sample': False, 'n_iter': 3000, 'alpha': 0.5, 'mu': mu_seq, 'bias': False, 'verbose': True}\n",
    "%time DDL.train_model(method='lr', lf_opts=lf_opts, model_opts=model_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.plot_calibration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxs, gt = DDL.get_labeled_ground_truth(subset=DDL.holdout())\n",
    "acc_feats = np.mean(DDL.get_predicted(subset=DDL.holdout()) == gt)\n",
    "acc_lfs = np.mean(DDL.get_lf_predicted(subset=DDL.holdout()) == gt)\n",
    "print \"LF accuracy: {:.3f}\\nFull model accuracy: {:.3f}\".format(acc_lfs, acc_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
